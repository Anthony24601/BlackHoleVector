
1. Convolutional Neural Networks (CNNs) Embeddings:
   - What is a CNN?
    deep learning model designed to process visual data. 
    don't treat every pixel in an image equally, 
    CNNs use filters (small sliding windows) that capture specific features such as edges, textures, and shapes. 
    Model focuses on patterns in the image.
   
   - How CNN Embeddings Work:
     CNNs process an image through multiple layers, 
     with each layer learning increasingly abstract features. 
     After passing through several layers, 
     the image is compressed into a lower-dimensional vector called an "embedding." 

   - Use Cases:
    image classification, similarity searches, and clustering. 
    Pretrained CNNs (like ResNet or VGG) can be fine-tuned to specific tasks or used directly for extracting image embeddings.

---

2. Self-Supervised Learning Models (e.g., SimCLR, MoCo, BYOL):
   - What is Self-Supervised Learning?
     Self-supervised learning models learn without the need for labels. 
     These models create their own "tasks" to learn from the data itself.

     I'm a little worries about the efficacy or speed of this however
        Over training is another potential issue
   
   - How Self-Supervised Models Work:
     Goal is to make two different augmented views of the same image similar in the embedding space, 
     while making embeddings of different images distinct. 
     For example, SimCLR takes an image, creates two random transformations (like cropping or flipping), 
     and trains the model to recognize that these two versions come from the same source. 
     The output is an embedding that captures the essence of the image.

     This I like, It will allow us to skip some preprocessing steps
        And we worry less about how transformations affect the embeddings

   - Use Cases:
     Self-supervised learning models are particularly useful when you have a large dataset without labels. 
     They produce embeddings that can be used for similarity searches or clustering, 
     even without manually categorizing each image.

---

3. Autoencoders:
   - What is an Autoencoder?
     An autoencoder is a type of neural network designed to compress data (like images) into a smaller representation 
     and then reconstruct it back to its original form. It has two main parts:
       1. Encoder: Compresses the image into a lower-dimensional vector (embedding).
       2. Decoder: Tries to recreate the original image from this compressed embedding.

    We would stop at step 1 here
   
   - How Autoencoders Work:
     The encoder learns to capture the most important features of the image and discard unnecessary details. 
     The resulting embedding represents a condensed version of the image. 
     After training, you can use the encoder’s output (the embedding) to compare images or group similar ones together.

   - Use Cases:
     Autoencoders are excellent for unsupervised learning tasks where the goal is to group similar images (clustering) 
     or find patterns without labels. 
     They're widely used in dimensionality reduction, image denoising, and anomaly detection.

---

4. Vision Transformers (ViTs):
   - What is a Transformer?
     Originally developed for language tasks (like translation)
     Processes data in parallel and can learn relationships between different parts of the input. 
     Treats the image as a sequence of patches rather than using filters like CNNs.
   
   - How Vision Transformers Work:
     ViTs divide the image into small patches and process these patches in parallel. 
     This creates a final embedding that captures the image's global structure and important features. The main difference from CNNs is that ViTs are not restricted to local filters (small sliding windows) but can learn long-range relationships within the image.

   - Use Cases:
     Used in image classification and embeddings
     they are highly competitive with CNNs. 
     They are particularly useful for capturing both fine details and the overall structure of the image.

---
Background Information:

1. Embeddings:
   - An embedding is a representation of an image as a vector (a list of numbers) in a lower-dimensional space. 
   Instead of representing an image by every pixel, embeddings capture the essential features of an image. 
   The main goal is to make images that are similar to each other have similar embeddings.

2. Similarity Search:
   - Similarity search involves finding images that are "close" to each other in the embedding space. By comparing the embeddings of different images, we can measure how similar they are. The closer the vectors (embeddings), the more similar the images.

3. Clustering:
   - Clustering groups similar images together based on their embeddings. It is often used when we don't have predefined categories (unsupervised learning). For example, you can cluster your black hole images into groups of "rings," "double rings," and "no rings" without manual labeling.




An autoencoder with 10 dims
    Fit gaussian to each cluster 
    Split the embedding space into many small spaces/models

    Separate the latent space

    Difference in some dimensions may affect L2 distances
        Embedding helps clustering by getting rid of noise within data

Preliminary embedding to get rid of high-dimensional noise

Black hole
    Spin 
    Initial condition
    Becomes an initial boundary problem

    mass and flux
        2 mass: density of the plasma and black hole mass

Embedding in row form
    Find out the dimensionless parameters that they correspond to


image pixel by pixel is highly dimensional data

We want to normalize the images beyond rotations and shifts


Longless of elipse
    Tilted/rotations

2 well defined clusters and see if there is any clusters




Autoencoder

Scalling in terms of cost of training
n parameters
    n2 is expensive

embedding alignment



1. Flat Index (IndexFlatL2): In a flat index, Faiss stores vectors in a list-like structure without additional
 preprocessing. During upserts, vectors are simply appended or replaced without extra computation. Searches in
  this index type are brute-force, meaning no clustering or compression is done ahead of time, so every distance
 calculation is handled at query time, making it straightforward but less optimized for very large datasets.

2. IVF Index (Inverted File Index): With an IVF index, Faiss first clusters the space into cells using a k-means
 clustering technique. When vectors are added or updated, Faiss assigns each vector to the nearest centroid, 
 effectively placing it in a specific cell within the index. This structure reduces search time by only searching 
 relevant cells, but adding or updating vectors still involves computing distances to centroids to find the 
 appropriate cell. The main precomputation (centroid calculation) happens when the index is created rather 
 than during each individual upsert.

3. Product Quantization (PQ) Index: Product quantization compresses vectors by splitting them into smaller 
sub-vectors and mapping each to a codebook entry (a cluster within each subspace). When upserting, Faiss 
finds the nearest codebook entry for each subspace, enabling the vector to be represented as a compact code. 
This approach reduces memory usage and query time but requires computing nearest sub-vectors during the upsert 
to assign them to the codebook entries. The PQ encoding itself is precomputed during index creation.

4. HNSW (Hierarchical Navigable Small World): In this graph-based index, Faiss updates the proximity graph 
when new vectors are added. The new vector connects to its closest neighbors, requiring Faiss to compute distances
to establish these relationships dynamically. No additional clustering or codebook assignment occurs here, but 
upserts do involve localized graph updates.




In a standard transformer, particularly the self-attention mechanism, dot product operations are responsible 
for a significant portion of the computational cost, often around 70% of the total time and energy used, 
depending on the model size, input length, and hardware.


Self-Attention Mechanism: Each token in an input sequence is compared to every other token in that sequence 
to compute attention scores. This requires a large number of dot products, scaling quadratically with 
sequence length O(n^2d) where n is the sequence length and d is the embedding dimension.

Scaling with Layers: Each layer of the transformer repeats this operation across multiple attention heads. 
For example, a model like BERT-base with 12 layers and 12 heads per layer performs a vast number of dot products 
at each layer, further compounding the computational cost.

Hardware and Memory Bandwidth: The dot product’s memory and compute requirements can outstrip the available 
bandwidth on most hardware, especially for long sequences and large batch sizes. This is a primary bottleneck 
in real-world deployment.

Optimization Efforts: Techniques like sparse attention, low-rank approximations, and hardware accelerators 
(like TPUs and GPUs optimized for matrix multiplications) have been introduced largely to mitigate the cost 
of these dot products by reducing their frequency or approximating the computations.







In transformer architectures, the self-attention mechanism is indeed one of the largest computational contributors,
typically making up about 70% of the overall processing cost. This high percentage is due to the quadratic scaling
of self-attention with respect to sequence length, as it involves computing attention weights for every token pair
, which becomes computationally intense for long sequences and large models.

"On the Computational Complexity of Self-Attention" (2023): This paper discusses why self-attention’s complexity 
is difficult to reduce below quadratic scaling and examines the use of approximations to mitigate some of these 
costs in large models


In the self-attention mechanism, the dot product calculation forms the core of the computational workload, 
typically representing around 80-90% of the self-attention's operations. This is because self-attention relies 
on computing attention scores for each token pair within a sequence, which is achieved by taking the dot product
 of query and key vectors, followed by a softmax operation and matrix multiplication with value vectors.

How it works:

Dot Product of Queries and Keys: This is the most computationally intensive part, as it requires a matrix 
multiplication that scales quadratically with sequence length (O(n2⋅d)), where nn is the sequence length 
and dd is the embedding dimension. This step alone accounts for the majority of self-attention’s computational cost.

Softmax and Weighted Sum with Value Vectors: After the dot products, the resulting attention scores are normalized 
with softmax and multiplied with value vectors. While essential, this step is computationally lighter than the dot 
product operations.

Therefore, the self-attention mechanism essentially revolves around these dot product calculations. The time and 
space complexity here underscores why self-attention is computationally expensive, prompting research into methods 
like sparse attention to reduce the impact of these dot products for large sequences



